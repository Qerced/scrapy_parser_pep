# Парсер документации Python PEP
Этот учебный проект представляет собой парсер документации Python PEP (Python Enhancement Proposals). Парсер разработан на базе фреймворка Scrapy и предназначен для сбора и обобщения информации о PEP.

## Технический стек
В проекте используются следующие технологии и инструменты:

- Язык программирования: Python 3.9
- Фреймворк для веб-парсинга: Scrapy
- Другие зависимости: [requirements.txt](./requirements.txt)

## Scrapy - фреймворк для веб-парсинга
Scrapy - это мощный фреймворк для извлечения данных из веб-сайтов. Он предоставляет удобный интерфейс для создания веб-пауков (спайдеров), которые позволяют автоматизировать процесс сбора информации с веб-страниц. Scrapy поддерживает различные методы для выбора данных, обхода страниц, обработки данных и сохранения результатов.

## Установка
Для работы с парсером необходимо установить Python версии 3.x и следующие зависимости:
```
pip install -r requirements.txt
```

## Запуск парсера
Для запуска парсера необходимо выполнить следующую команду:
```
scrapy crawl pep
```
Здесь pep - это название спайдера, который содержит логику для обхода страниц и извлечения данных о PEP.
После успешного выполнения, парсер соберет информацию и сохранит ее в файл 'pep_%(time)s.csv' и 'status_summary_%(time)s.csv'.

## Структура сохраненных данных
### pep.csv
Файл pep.csv содержит информацию о существующих PEP:
- номер PEP.
- название PEP.
- статус PEP (например, Accepted, Rejected, Final, Draft и т.д.).

### status_summary.csv
Файл status_summary.csv содержит сводную информацию о количестве PEP в каждом из существующих статусов и общее количество PEP:
- статус PEP.
- количество PEP с данным статусом.
- общее количество PEP.

## Авторы:
- [Vakauskas Vitas](https://github.com/Qerced)
